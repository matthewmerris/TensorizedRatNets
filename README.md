# TensorizedRatNets
Treating the analysis of neural network activations as a blind signal separation problem by exploiting tensorization and rational network activations.
This approach is made possible owing to rational function approximation of the popular ReLU activations function.  Employing rational function approximation for ReLU in neural networks allows a network's activations to be deterministically transformed into tensors via a process called L\"ownerization.
